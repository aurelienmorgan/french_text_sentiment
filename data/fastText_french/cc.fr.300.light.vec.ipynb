{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, re, time\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(os.path.realpath(\"..\\..\")) # Adds higher directory to python modules path.\n",
    "from my_NLP_RNN_fr_lib.tweet_utils import load_review_fr, load_tweet_fr, \\\n",
    "                                    clean_tweet_fr, TWEET_MAX_CHAR_COUNT\n",
    "from my_NLP_RNN_fr_lib.tokenizer_utils import tokenize_tweet_fr\n",
    "from my_NLP_RNN_fr_lib.fastText import read_fastText_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "In the herein short notebook, we load the french-text fastText dictionnary we use in the global context of our NLP model use case. What we especially do, here, is \"shrink\" the vocabs so that it includes only tokens that do exist is our training dataset. By doing that, we lower the footprint of our RNN embeddings layer in memory.\n",
    "</div>\n",
    "<br />\n",
    "<div style=\"text-align: justify\">\n",
    "BEWARE&nbsp;: We only use the \"shrinked\" version of our vocab during the model optimization stage. If we were to use that version of the french vocab in production, it would totally annihilate the \"generalizing\" power of our NLP model. It would in such circumstances be unable to generalize to words it hadn't seen during training time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by identifying all word tokens appearing in our transfer-learning \"French Reviews\" dataset&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 0.8286 seconds\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "reviews = load_review_fr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "reviews = clean_tweet_fr(reviews, col_name = 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenized_reviews = tokenize_tweet_fr(reviews, col_name='comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering-out lengthy elements (if any)\n",
    "tokenized_reviews.drop( tokenized_reviews[tokenized_reviews.map(\n",
    "    lambda tokens: len(tokens)) > TWEET_MAX_CHAR_COUNT].index, inplace=True )\n",
    "tokenized_reviews.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173,014 reviews.\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\".format(tokenized_reviews.shape[0]) + \" reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening\n",
    "reviews_tokens = tokenized_reviews.apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"{:,}\".format(len(reviews_tokens))": "7,588,633"
    }
   },
   "source": [
    "Our \"French Reviews\" training dataset encapsulates {{\"{:,}\".format(len(reviews_tokens))}} (non-unique) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, garbage collection\n",
    "try : del reviews, tokenized_reviews ; dummy = gc.collect()\n",
    "except NameError : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now do the same with our taget \"French Tweets\" dataset by identifying all word tokens appearing in there&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 2.8951 seconds\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "tweets = load_tweet_fr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tweets = clean_tweet_fr(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenized_tweets = tokenize_tweet_fr(tweets)\n",
    "#del tweets ; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering-out lengthy elements (if any)\n",
    "tokenized_tweets.drop( tokenized_tweets[tokenized_tweets.map(\n",
    "    lambda tokens: len(tokens)) > TWEET_MAX_CHAR_COUNT].index, inplace=True )\n",
    "tokenized_tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening\n",
    "tweets_tokens = tokenized_tweets.apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"{:,}\".format(len(tweets_tokens))": "5,385,103"
    }
   },
   "source": [
    "Our \"French Tweets\" training dataset encapsulates {{\"{:,}\".format(len(tweets_tokens))}} (non-unique) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, garbage collection\n",
    "try : del tweets, tokenized_tweets ; silent = gc.collect()\n",
    "except : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step now consists in merging the two sub-ensembles and extract a list of unique tokens&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pd.concat([reviews_tokens.astype(\"str\"), tweets_tokens.astype(\"str\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens.reset_index(drop=True).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"{:,}\".format(len(tokens))": "176,434"
    }
   },
   "source": [
    "Our entire training dataset encapsulates <b><u>{{\"{:,}\".format(len(tokens))}}</u></b> unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, garbage collection\n",
    "try : del reviews_tokens, tweets_tokens ; gc.collect()\n",
    "except : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning a 'dict' object into a DataFrame\n",
    "# for further manipulating down the road\n",
    "# (dict_keys using unpacking => [*tokens])\n",
    "tokens_df = pd.DataFrame([*tokens])\n",
    "tokens_df.columns=['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>Une</td>\n",
       "      <td>cuisine</td>\n",
       "      <td>japonaise</td>\n",
       "      <td>sans</td>\n",
       "      <td>originalités</td>\n",
       "      <td>particulières</td>\n",
       "      <td>mais</td>\n",
       "      <td>la</td>\n",
       "      <td>meilleure</td>\n",
       "      <td>du</td>\n",
       "      <td>genre</td>\n",
       "      <td>.</td>\n",
       "      <td>Pour</td>\n",
       "      <td>de</td>\n",
       "      <td>tels</td>\n",
       "      <td>prix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2     3             4              5     6   7  \\\n",
       "tokens  Une  cuisine  japonaise  sans  originalités  particulières  mais  la   \n",
       "\n",
       "                8   9     10 11    12  13    14    15  \n",
       "tokens  meilleure  du  genre  .  Pour  de  tels  prix  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display a sample of the training unique tokens\n",
    "tokens_df[0:16].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logical following step consists in loading our original full-size vocabulary&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the Vocabulary in 177.9774 seconds [1,234,042 word embeddings of 300 features]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = \\\n",
    "    read_fastText_vecs( os.path.join(\n",
    "        os.path.realpath(\"..\\..\")\n",
    "        , 'data', 'fastText_french', 'cc.fr.300.vec') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning a 'dict' object into a DataFrame\n",
    "# for further manipulating down the road\n",
    "# (dict_keys using unpacking => [*word_to_index])\n",
    "vocab_df = pd.DataFrame([*word_to_index])\n",
    "vocab_df.columns=['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>!</td>\n",
       "      <td>\"</td>\n",
       "      <td>#</td>\n",
       "      <td>$</td>\n",
       "      <td>%</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>'</td>\n",
       "      <td>(</td>\n",
       "      <td>)</td>\n",
       "      <td>*</td>\n",
       "      <td>+</td>\n",
       "      <td>,</td>\n",
       "      <td>-</td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1  2  3  4  5  6  7  8  9 10 11 12 13   14 15\n",
       "tokens  !  \"  #  $  %  &  '  (  )  *  +  ,  -  .  ...  /"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display a sample of the vocabulary tokens\n",
    "vocab_df[0:16].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Let us join the set of unique training tokens with our vocabulary, to only retain the intersection. Notice below how some of the tokens of our training dataset are absent from the original full-size french vocabulary. Most of these are indeed misspelled words. For such tokens, we adjunct an \"<em>&lt;UKN&gt;</em>\" (\"unknown\") token to the vocabulary when used in conjunction with an NLP model &nbsp;:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <!DOCTYPE html><html><head>\n",
       "        <style>\n",
       "        div.centered_table_container {\n",
       "            text-align: center;\n",
       "        }\n",
       "        table {\n",
       "            display: inline-block;\n",
       "        }\n",
       "        </style></head>\n",
       "        <body>\n",
       "        <div class=\"centered_table_container\"><style  type=\"text/css\" >\n",
       "    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2 th {\n",
       "          text-align: center;\n",
       "    }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row0_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row1_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row2_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row3_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row4_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row5_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row6_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row7_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row8_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row9_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row10_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row11_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row12_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row13_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row14_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }    #T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row15_col0 {\n",
       "            width:  20em;\n",
       "            text-align:  center;\n",
       "        }</style><table id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >tokens</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row0_col0\" class=\"data row0 col0\" >b&acirc;teux</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row1_col0\" class=\"data row1 col0\" >al&eacute;se</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row2_col0\" class=\"data row2 col0\" >reviderai</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row3_col0\" class=\"data row3 col0\" >thecniquement</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row4_col0\" class=\"data row4 col0\" >impretioner</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row5_col0\" class=\"data row5 col0\" >girardet</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row6_col0\" class=\"data row6 col0\" >passedat</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row7_col0\" class=\"data row7 col0\" >disct&eacute;tion</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row8_col0\" class=\"data row8 col0\" >fellini</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row9_col0\" class=\"data row9 col0\" >Marqueting</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row10_col0\" class=\"data row10 col0\" >sshhhhhhut</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row11_col0\" class=\"data row11 col0\" >tr&Atilde;&scaron;s</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row12_col0\" class=\"data row12 col0\" >driessche</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row13_col0\" class=\"data row13 col0\" >oppr&eacute;ssante</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row14_col0\" class=\"data row14 col0\" >Kounmandou</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_3f700a92_eaa8_11ea_8e23_309c23ff89f2row15_col0\" class=\"data row15 col0\" >Bibimpap</td>\n",
       "            </tr>\n",
       "    </tbody></table></div></body></html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_NLP_RNN_fr_lib.tweet_utils import html_escape\n",
    "from my_NLP_RNN_fr_lib.display_helper import dataframe_pretty_print_center\n",
    "\n",
    "# sample of tokens in our datasets BUT not in our vocab =>\n",
    "result_df = \\\n",
    "    pd.DataFrame({'tokens' :\n",
    "                  [html_escape(token) for token in tokens_df.tokens[~tokens_df.tokens.isin(vocab_df.tokens)].values[0:16]]})\n",
    "\n",
    "dataframe_pretty_print_center(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDING VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(word_to_vec_map['bonjour'])": "300"
    }
   },
   "source": [
    "Finally, we can now retrieve the filtered word vector coordinates in the space of {{len(word_to_vec_map['bonjour'])}} features&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map_light = \\\n",
    "    dict((k, word_to_vec_map[k])\n",
    "        for k in tokens_df.tokens[tokens_df.tokens.isin(vocab_df.tokens)]\n",
    "         if k in word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "\"{:,}\".format(len(word_to_vec_map_light))": "129,064"
    }
   },
   "source": [
    "<div style=\"text-align: justify\">\n",
    "    There are <b><u>{{\"{:,}\".format(len(word_to_vec_map_light))}}</u></b> entries in the light version of the dictionnary that we just created. Notice again how this represent less than the number of tokens of our training dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, it is at last possible to record a local \"light\" version of the original french vocabulary&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the LIGHT Vocabulary locally in 22.6517 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "str_list = []\n",
    "counter = 0\n",
    "with open(\n",
    "    os.path.join(\n",
    "        os.path.realpath(\"..\\..\")\n",
    "        , 'data', 'fastText_french', 'cc.fr.300.light.vec'\n",
    "    ) , 'w', encoding=\"utf-8\") as f:\n",
    "    for token, vector in word_to_vec_map_light.items():\n",
    "        str_list.append(str(token) + \" \" + ' '.join(str(x) for x in vector) + '\\n')\n",
    "        counter += 1\n",
    "        if counter % 50000 == 0 :\n",
    "            f.write(''.join(str_list))\n",
    "            str_list = []\n",
    "    if counter % 50000 != 0 :\n",
    "        f.write(''.join(str_list))\n",
    "toc = time.perf_counter()\n",
    "print(f\"Saved the LIGHT Vocabulary locally in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 4px; width: 70%; margin:0 auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so-created vocab can then be loaded the standard way as follows&nbsp;:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div style=\"background-color: #f0f0f0; width: 600px; text-align: left;\"><code>word_to_index_light, index_to_word_light, word_to_vec_map_light = \\\n",
    "    read_fastText_vecs( os.path.join(\n",
    "        os.path.realpath(\"..\\..\")\n",
    "    , 'data', 'fastText_french', 'cc.fr.300<b>.light</b>.vec') )\n",
    "</code></div></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13.1",
   "language": "python",
   "name": "r-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
